{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e3f8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_utils' from 'C:\\\\Users\\\\ysman\\\\Desktop\\\\Capstone\\\\PIVOT\\\\../ml-workflow\\\\data_utils.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import getpass\n",
    "\n",
    "import azure.storage.blob\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import sys\n",
    "sys.path.append('../ml-workflow/') # point to wherever the ml-workflow directory is\n",
    "import imp\n",
    "\n",
    "import data_utils as du\n",
    "import ml_models\n",
    "\n",
    "from tqdm.auto import trange, tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "# Need to comment out gray_image = cv2.cvtColor(ri, cv2.COLOR_BGR2GRAY) in du.prepocess_input\n",
    "# gray_image = ri\n",
    "from importlib import reload\n",
    "reload(du)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e009da",
   "metadata": {},
   "source": [
    "## Current Azure API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d91e2621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DefaultEndpointsProtocol=https;AccountName=ifcb;AccountKey=mzeoQjV8g8RZyI18AdD4x1XxQRm0GesvssNtPgGNom7a8Kk3FRe8bjCm9BRq7m/gwqrK8GcwOXDBPKWq21jFPg==;EndpointSuffix=core.windows.net'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"DefaultEndpointsProtocol=https;AccountName=ifcb;AccountKey=mzeoQjV8g8RZyI18AdD4x1XxQRm0GesvssNtPgGNom7a8Kk3FRe8bjCm9BRq7m/gwqrK8GcwOXDBPKWq21jFPg==;EndpointSuffix=core.windows.net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34a9f178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "connection_string = getpass.getpass()\n",
    "# copy the above string without any quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58e39684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container_client = azure.storage.blob.ContainerClient.from_connection_string(\n",
    "    connection_string, container_name=\"naames\"\n",
    ")\n",
    "# this will return True if the authentication works.\n",
    "# if not, then it will show as [*] running for a while without any output\n",
    "container_client.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76043800",
   "metadata": {},
   "source": [
    "## Data Source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc21db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "container_name = 'naames'\n",
    "account_name = 'ifcb'\n",
    "url_prefix = f\"https://{account_name}.blob.core.windows.net/{container_name}/\"\n",
    "\n",
    "# csv_url = url_prefix + 'image-file-directory.csv'\n",
    "csv_url = url_prefix + 'NAAMES-predicted-labels-model-cnn-v1-b3.csv'\n",
    "inventory_df = pd.read_csv(csv_url)\n",
    "\n",
    "len(inventory_df['pred_label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ee3a3",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# Load the model architecture from JSON file\n",
    "json_file_path = 'ml-workflow/model_ckpt/model-cnn-v1-b3.json'\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# Load the model weights from H5 file\n",
    "h5_file_path = 'ml-workflow/model_ckpt/model-cnn-v1-b3.h5'\n",
    "loaded_model.load_weights(h5_file_path)\n",
    "\n",
    "# Compile the loaded model if needed\n",
    "# loaded_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Now the 'loaded_model' contains both architecture and weights\n",
    "\n",
    "# Example: Make predictions using the loaded model\n",
    "# predictions = loaded_model.predict(your_input_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d10b4a",
   "metadata": {},
   "source": [
    "## Parallelized Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191940ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the batch size and number of workers\n",
    "batch_size = 320\n",
    "num_workers = 24  # Adjust based on your system's capabilities\n",
    "\n",
    "# Split URLs into batches\n",
    "start_index = 0\n",
    "url_batches = [cloud_urls[i:i + batch_size] for i in range(start_index, len(cloud_urls)//5, batch_size)]\n",
    "\n",
    "# Create an empty list to store probabilities\n",
    "all_probabilities = []\n",
    "\n",
    "# Create a list to store the order of URLs\n",
    "url_order = []\n",
    "\n",
    "# Define a function for parallel preprocessing\n",
    "def preprocess_batch(order_index, batch_urls):\n",
    "    idxs = []\n",
    "    data = []\n",
    "    for i, c_url in enumerate(tqdm(batch_urls, leave=False)):\n",
    "        data.append(du.preprocess_input(np.expand_dims(imageio.v2.imread(c_url), axis=-1)))\n",
    "        idxs.append(order_index*batch_size + i + start_index)\n",
    "    \n",
    "    result = (idxs, data)\n",
    "    return result\n",
    "\n",
    "try:\n",
    "    # Use concurrent.futures for parallel processing\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        future_to_order_index = {\n",
    "            executor.submit(preprocess_batch, i, batch_urls): i \n",
    "            for i, batch_urls in enumerate(url_batches)\n",
    "        }\n",
    "        for future in tqdm(concurrent.futures.as_completed(future_to_order_index), total=len(url_batches)):\n",
    "            order_index = future_to_order_index[future]\n",
    "            try:\n",
    "                # Get batch of images\n",
    "                url_idxs, img_data = future.result()\n",
    "                url_order.extend(url_idxs)\n",
    "                \n",
    "                # predict images\n",
    "                batch_probabilities = loaded_model.predict(np.array(img_data), verbose=0)\n",
    "                all_probabilities.extend(batch_probabilities)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {order_index}: {e}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Process interrupted by user.\")\n",
    "\n",
    "# Sort results based on the original order of URLs\n",
    "sorted_indices = np.argsort(url_order)\n",
    "ordered_all_probs = np.vstack(all_probabilities)[sorted_indices]\n",
    "# Now, 'ordered_all_probs' contains the concatenated probabilities for all URLs in the original order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f6ff41",
   "metadata": {},
   "source": [
    "## Serial Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf6d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 320\n",
    "all_probabilities = []\n",
    "for i in trange(0,len(cloud_urls), batch_size):\n",
    "    images = np.array([\n",
    "        du.preprocess_input(np.expand_dims(imageio.v2.imread(c_url), axis=-1)) for c_url in tqdm(cloud_urls[i:i+batch_size])\n",
    "    ])\n",
    "    probabilities = loaded_model.predict(images)\n",
    "    all_probabilities.append(probabilities)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pivot-modAL",
   "language": "python",
   "name": "pivot-modal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
